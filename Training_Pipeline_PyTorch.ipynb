{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9cf2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8b0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8681b35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5016f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id','Unnamed: 32'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b63986",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a362a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.69</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>19.270</td>\n",
       "      <td>26.47</td>\n",
       "      <td>127.90</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.17190</td>\n",
       "      <td>0.16570</td>\n",
       "      <td>0.07593</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.06261</td>\n",
       "      <td>...</td>\n",
       "      <td>24.15</td>\n",
       "      <td>30.90</td>\n",
       "      <td>161.40</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>0.1509</td>\n",
       "      <td>0.6590</td>\n",
       "      <td>0.60910</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.3672</td>\n",
       "      <td>0.11230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.20770</td>\n",
       "      <td>0.09756</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>18.07</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.5954</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>13.370</td>\n",
       "      <td>16.39</td>\n",
       "      <td>86.10</td>\n",
       "      <td>553.5</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.08092</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.05823</td>\n",
       "      <td>...</td>\n",
       "      <td>14.26</td>\n",
       "      <td>22.75</td>\n",
       "      <td>91.99</td>\n",
       "      <td>632.1</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.33080</td>\n",
       "      <td>0.08978</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.07628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>12.700</td>\n",
       "      <td>12.17</td>\n",
       "      <td>80.88</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0.08785</td>\n",
       "      <td>0.05794</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>0.02402</td>\n",
       "      <td>0.1583</td>\n",
       "      <td>0.06275</td>\n",
       "      <td>...</td>\n",
       "      <td>13.65</td>\n",
       "      <td>16.92</td>\n",
       "      <td>88.12</td>\n",
       "      <td>566.9</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.1607</td>\n",
       "      <td>0.09385</td>\n",
       "      <td>0.08224</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.09464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>11.060</td>\n",
       "      <td>14.96</td>\n",
       "      <td>71.49</td>\n",
       "      <td>373.9</td>\n",
       "      <td>0.10330</td>\n",
       "      <td>0.09097</td>\n",
       "      <td>0.05397</td>\n",
       "      <td>0.03341</td>\n",
       "      <td>0.1776</td>\n",
       "      <td>0.06907</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>19.90</td>\n",
       "      <td>79.76</td>\n",
       "      <td>440.0</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.22990</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.09080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>13.380</td>\n",
       "      <td>30.72</td>\n",
       "      <td>86.34</td>\n",
       "      <td>557.2</td>\n",
       "      <td>0.09245</td>\n",
       "      <td>0.07426</td>\n",
       "      <td>0.02819</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.06016</td>\n",
       "      <td>...</td>\n",
       "      <td>15.05</td>\n",
       "      <td>41.61</td>\n",
       "      <td>96.69</td>\n",
       "      <td>705.6</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.07003</td>\n",
       "      <td>0.07763</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>12.830</td>\n",
       "      <td>15.73</td>\n",
       "      <td>82.89</td>\n",
       "      <td>506.9</td>\n",
       "      <td>0.09040</td>\n",
       "      <td>0.08269</td>\n",
       "      <td>0.05835</td>\n",
       "      <td>0.03078</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>...</td>\n",
       "      <td>14.09</td>\n",
       "      <td>19.35</td>\n",
       "      <td>93.22</td>\n",
       "      <td>605.8</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.3006</td>\n",
       "      <td>0.07802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>9.876</td>\n",
       "      <td>17.27</td>\n",
       "      <td>62.92</td>\n",
       "      <td>295.4</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.07232</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.01952</td>\n",
       "      <td>0.1934</td>\n",
       "      <td>0.06285</td>\n",
       "      <td>...</td>\n",
       "      <td>10.42</td>\n",
       "      <td>23.22</td>\n",
       "      <td>67.08</td>\n",
       "      <td>331.6</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>0.06213</td>\n",
       "      <td>0.05588</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>0.07380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>9.405</td>\n",
       "      <td>21.70</td>\n",
       "      <td>59.60</td>\n",
       "      <td>271.2</td>\n",
       "      <td>0.10440</td>\n",
       "      <td>0.06159</td>\n",
       "      <td>0.02047</td>\n",
       "      <td>0.01257</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.06601</td>\n",
       "      <td>...</td>\n",
       "      <td>10.85</td>\n",
       "      <td>31.24</td>\n",
       "      <td>68.73</td>\n",
       "      <td>359.4</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.06141</td>\n",
       "      <td>0.03770</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.08304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "33        19.270         26.47          127.90     1162.0          0.09401   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "124       13.370         16.39           86.10      553.5          0.07115   \n",
       "418       12.700         12.17           80.88      495.0          0.08785   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "342       11.060         14.96           71.49      373.9          0.10330   \n",
       "455       13.380         30.72           86.34      557.2          0.09245   \n",
       "475       12.830         15.73           82.89      506.9          0.09040   \n",
       "206        9.876         17.27           62.92      295.4          0.10890   \n",
       "416        9.405         21.70           59.60      271.2          0.10440   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "33            0.17190         0.16570              0.07593         0.1853   \n",
       "22            0.21350         0.20770              0.09756         0.2521   \n",
       "124           0.07325         0.08092              0.02800         0.1422   \n",
       "418           0.05794         0.02360              0.02402         0.1583   \n",
       "..                ...             ...                  ...            ...   \n",
       "342           0.09097         0.05397              0.03341         0.1776   \n",
       "455           0.07426         0.02819              0.03264         0.1375   \n",
       "475           0.08269         0.05835              0.03078         0.1705   \n",
       "206           0.07232         0.01756              0.01952         0.1934   \n",
       "416           0.06159         0.02047              0.01257         0.2025   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "565                 0.05533  ...         23.69          38.25   \n",
       "33                  0.06261  ...         24.15          30.90   \n",
       "22                  0.07032  ...         18.07          19.08   \n",
       "124                 0.05823  ...         14.26          22.75   \n",
       "418                 0.06275  ...         13.65          16.92   \n",
       "..                      ...  ...           ...            ...   \n",
       "342                 0.06907  ...         11.92          19.90   \n",
       "455                 0.06016  ...         15.05          41.61   \n",
       "475                 0.05913  ...         14.09          19.35   \n",
       "206                 0.06285  ...         10.42          23.22   \n",
       "416                 0.06601  ...         10.85          31.24   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "565           155.00      1731.0            0.1166             0.1922   \n",
       "33            161.40      1813.0            0.1509             0.6590   \n",
       "22            125.10       980.9            0.1390             0.5954   \n",
       "124            91.99       632.1            0.1025             0.2531   \n",
       "418            88.12       566.9            0.1314             0.1607   \n",
       "..               ...         ...               ...                ...   \n",
       "342            79.76       440.0            0.1418             0.2210   \n",
       "455            96.69       705.6            0.1172             0.1421   \n",
       "475            93.22       605.8            0.1326             0.2610   \n",
       "206            67.08       331.6            0.1415             0.1247   \n",
       "416            68.73       359.4            0.1526             0.1193   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "565          0.32150               0.16280          0.2572   \n",
       "33           0.60910               0.17850          0.3672   \n",
       "22           0.63050               0.23930          0.4667   \n",
       "124          0.33080               0.08978          0.2048   \n",
       "418          0.09385               0.08224          0.2775   \n",
       "..               ...                   ...             ...   \n",
       "342          0.22990               0.10750          0.3301   \n",
       "455          0.07003               0.07763          0.2196   \n",
       "475          0.34760               0.09783          0.3006   \n",
       "206          0.06213               0.05588          0.2989   \n",
       "416          0.06141               0.03770          0.2872   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "565                  0.06637  \n",
       "33                   0.11230  \n",
       "22                   0.09946  \n",
       "124                  0.07628  \n",
       "418                  0.09464  \n",
       "..                       ...  \n",
       "342                  0.09080  \n",
       "455                  0.07675  \n",
       "475                  0.07802  \n",
       "206                  0.07380  \n",
       "416                  0.08304  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88a2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565    M\n",
       "33     M\n",
       "22     M\n",
       "124    B\n",
       "418    B\n",
       "      ..\n",
       "342    B\n",
       "455    B\n",
       "475    B\n",
       "206    B\n",
       "416    B\n",
       "Name: diagnosis, Length: 455, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c16eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f668b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59a052dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9566ae46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7f03c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb3cf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self,input_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_features,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,features):\n",
    "        return self.network(features)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba8b1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e73038",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss() # Binary Cross Entropy Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 , loss : 0.6911898851394653\n",
      "Epoch : 2 , loss : 0.6890465617179871\n",
      "Epoch : 3 , loss : 0.6870332956314087\n",
      "Epoch : 4 , loss : 0.6851416230201721\n",
      "Epoch : 5 , loss : 0.6833625435829163\n",
      "Epoch : 6 , loss : 0.6816893815994263\n",
      "Epoch : 7 , loss : 0.6801152229309082\n",
      "Epoch : 8 , loss : 0.6786340475082397\n",
      "Epoch : 9 , loss : 0.6772395968437195\n",
      "Epoch : 10 , loss : 0.6759260296821594\n",
      "Epoch : 11 , loss : 0.6746886968612671\n",
      "Epoch : 12 , loss : 0.6735230088233948\n",
      "Epoch : 13 , loss : 0.67242431640625\n",
      "Epoch : 14 , loss : 0.6713884472846985\n",
      "Epoch : 15 , loss : 0.6704114079475403\n",
      "Epoch : 16 , loss : 0.6694900393486023\n",
      "Epoch : 17 , loss : 0.6686207056045532\n",
      "Epoch : 18 , loss : 0.66780024766922\n",
      "Epoch : 19 , loss : 0.6670259833335876\n",
      "Epoch : 20 , loss : 0.6662951111793518\n",
      "Epoch : 21 , loss : 0.6656050086021423\n",
      "Epoch : 22 , loss : 0.664953351020813\n",
      "Epoch : 23 , loss : 0.6643377542495728\n",
      "Epoch : 24 , loss : 0.6637561917304993\n",
      "Epoch : 25 , loss : 0.6632066369056702\n",
      "Epoch : 26 , loss : 0.6626874208450317\n",
      "Epoch : 27 , loss : 0.6621965169906616\n",
      "Epoch : 28 , loss : 0.6617324352264404\n",
      "Epoch : 29 , loss : 0.661293625831604\n",
      "Epoch : 30 , loss : 0.6608786582946777\n",
      "Epoch : 31 , loss : 0.660486102104187\n",
      "Epoch : 32 , loss : 0.6601147055625916\n",
      "Epoch : 33 , loss : 0.6597634553909302\n",
      "Epoch : 34 , loss : 0.6594309210777283\n",
      "Epoch : 35 , loss : 0.6591162085533142\n",
      "Epoch : 36 , loss : 0.6588183045387268\n",
      "Epoch : 37 , loss : 0.6585362553596497\n",
      "Epoch : 38 , loss : 0.6582691073417664\n",
      "Epoch : 39 , loss : 0.6580159664154053\n",
      "Epoch : 40 , loss : 0.6577762961387634\n",
      "Epoch : 41 , loss : 0.6575492024421692\n",
      "Epoch : 42 , loss : 0.6573339700698853\n",
      "Epoch : 43 , loss : 0.6571298837661743\n",
      "Epoch : 44 , loss : 0.6569364070892334\n",
      "Epoch : 45 , loss : 0.65675288438797\n",
      "Epoch : 46 , loss : 0.6565788984298706\n",
      "Epoch : 47 , loss : 0.656413733959198\n",
      "Epoch : 48 , loss : 0.6562569737434387\n",
      "Epoch : 49 , loss : 0.6561082005500793\n",
      "Epoch : 50 , loss : 0.6559669375419617\n",
      "Epoch : 51 , loss : 0.6558327078819275\n",
      "Epoch : 52 , loss : 0.6557052731513977\n",
      "Epoch : 53 , loss : 0.6555840969085693\n",
      "Epoch : 54 , loss : 0.6554688811302185\n",
      "Epoch : 55 , loss : 0.6553592681884766\n",
      "Epoch : 56 , loss : 0.6552550196647644\n",
      "Epoch : 57 , loss : 0.6551556587219238\n",
      "Epoch : 58 , loss : 0.6550610661506653\n",
      "Epoch : 59 , loss : 0.6549710035324097\n",
      "Epoch : 60 , loss : 0.6548851132392883\n",
      "Epoch : 61 , loss : 0.6548031568527222\n",
      "Epoch : 62 , loss : 0.6547249555587769\n",
      "Epoch : 63 , loss : 0.6546503305435181\n",
      "Epoch : 64 , loss : 0.6545789837837219\n",
      "Epoch : 65 , loss : 0.6545106768608093\n",
      "Epoch : 66 , loss : 0.6544453501701355\n",
      "Epoch : 67 , loss : 0.6543827652931213\n",
      "Epoch : 68 , loss : 0.6543229222297668\n",
      "Epoch : 69 , loss : 0.6542654633522034\n",
      "Epoch : 70 , loss : 0.6542101502418518\n",
      "Epoch : 71 , loss : 0.6541571617126465\n",
      "Epoch : 72 , loss : 0.6541061401367188\n",
      "Epoch : 73 , loss : 0.6540569067001343\n",
      "Epoch : 74 , loss : 0.6540095210075378\n",
      "Epoch : 75 , loss : 0.6539638042449951\n",
      "Epoch : 76 , loss : 0.6539196372032166\n",
      "Epoch : 77 , loss : 0.6538770198822021\n",
      "Epoch : 78 , loss : 0.6538357138633728\n",
      "Epoch : 79 , loss : 0.6537957787513733\n",
      "Epoch : 80 , loss : 0.6537570357322693\n",
      "Epoch : 81 , loss : 0.6537192463874817\n",
      "Epoch : 82 , loss : 0.6536824703216553\n",
      "Epoch : 83 , loss : 0.6536467671394348\n",
      "Epoch : 84 , loss : 0.6536117196083069\n",
      "Epoch : 85 , loss : 0.6535777449607849\n",
      "Epoch : 86 , loss : 0.6535447239875793\n",
      "Epoch : 87 , loss : 0.6535124778747559\n",
      "Epoch : 88 , loss : 0.6534807682037354\n",
      "Epoch : 89 , loss : 0.653449535369873\n",
      "Epoch : 90 , loss : 0.6534188389778137\n",
      "Epoch : 91 , loss : 0.6533886194229126\n",
      "Epoch : 92 , loss : 0.6533586978912354\n",
      "Epoch : 93 , loss : 0.653329074382782\n",
      "Epoch : 94 , loss : 0.653299868106842\n",
      "Epoch : 95 , loss : 0.6532711386680603\n",
      "Epoch : 96 , loss : 0.6532426476478577\n",
      "Epoch : 97 , loss : 0.6532145142555237\n",
      "Epoch : 98 , loss : 0.6531866788864136\n",
      "Epoch : 99 , loss : 0.6531590223312378\n",
      "Epoch : 100 , loss : 0.6531314253807068\n",
      "Epoch : 101 , loss : 0.6531039476394653\n",
      "Epoch : 102 , loss : 0.6530765891075134\n",
      "Epoch : 103 , loss : 0.6530494689941406\n",
      "Epoch : 104 , loss : 0.6530224084854126\n",
      "Epoch : 105 , loss : 0.6529951691627502\n",
      "Epoch : 106 , loss : 0.6529678702354431\n",
      "Epoch : 107 , loss : 0.6529403924942017\n",
      "Epoch : 108 , loss : 0.6529127359390259\n",
      "Epoch : 109 , loss : 0.652884840965271\n",
      "Epoch : 110 , loss : 0.652856707572937\n",
      "Epoch : 111 , loss : 0.6528283953666687\n",
      "Epoch : 112 , loss : 0.6527996063232422\n",
      "Epoch : 113 , loss : 0.6527705788612366\n",
      "Epoch : 114 , loss : 0.6527413725852966\n",
      "Epoch : 115 , loss : 0.6527117490768433\n",
      "Epoch : 116 , loss : 0.6526818871498108\n",
      "Epoch : 117 , loss : 0.6526514291763306\n",
      "Epoch : 118 , loss : 0.6526211500167847\n",
      "Epoch : 119 , loss : 0.6525909304618835\n",
      "Epoch : 120 , loss : 0.6525599956512451\n",
      "Epoch : 121 , loss : 0.6525285243988037\n",
      "Epoch : 122 , loss : 0.6524968147277832\n",
      "Epoch : 123 , loss : 0.6524646282196045\n",
      "Epoch : 124 , loss : 0.6524319052696228\n",
      "Epoch : 125 , loss : 0.6523983478546143\n",
      "Epoch : 126 , loss : 0.6523639559745789\n",
      "Epoch : 127 , loss : 0.6523280739784241\n",
      "Epoch : 128 , loss : 0.6522913575172424\n",
      "Epoch : 129 , loss : 0.6522543430328369\n",
      "Epoch : 130 , loss : 0.6522167325019836\n",
      "Epoch : 131 , loss : 0.6521781086921692\n",
      "Epoch : 132 , loss : 0.6521386504173279\n",
      "Epoch : 133 , loss : 0.6520983576774597\n",
      "Epoch : 134 , loss : 0.6520568132400513\n",
      "Epoch : 135 , loss : 0.6520143151283264\n",
      "Epoch : 136 , loss : 0.6519712209701538\n",
      "Epoch : 137 , loss : 0.6519273519515991\n",
      "Epoch : 138 , loss : 0.6518824696540833\n",
      "Epoch : 139 , loss : 0.6518368124961853\n",
      "Epoch : 140 , loss : 0.6517909169197083\n",
      "Epoch : 141 , loss : 0.6517447829246521\n",
      "Epoch : 142 , loss : 0.6516965627670288\n",
      "Epoch : 143 , loss : 0.6516479253768921\n",
      "Epoch : 144 , loss : 0.6515979766845703\n",
      "Epoch : 145 , loss : 0.6515468955039978\n",
      "Epoch : 146 , loss : 0.6514950394630432\n",
      "Epoch : 147 , loss : 0.651441752910614\n",
      "Epoch : 148 , loss : 0.651387095451355\n",
      "Epoch : 149 , loss : 0.6513301134109497\n",
      "Epoch : 150 , loss : 0.6512711644172668\n",
      "Epoch : 151 , loss : 0.6512112021446228\n",
      "Epoch : 152 , loss : 0.6511498689651489\n",
      "Epoch : 153 , loss : 0.6510864496231079\n",
      "Epoch : 154 , loss : 0.6510214805603027\n",
      "Epoch : 155 , loss : 0.65095454454422\n",
      "Epoch : 156 , loss : 0.650885283946991\n",
      "Epoch : 157 , loss : 0.6508134603500366\n",
      "Epoch : 158 , loss : 0.650739312171936\n",
      "Epoch : 159 , loss : 0.6506640911102295\n",
      "Epoch : 160 , loss : 0.6505879759788513\n",
      "Epoch : 161 , loss : 0.6505100131034851\n",
      "Epoch : 162 , loss : 0.6504296660423279\n",
      "Epoch : 163 , loss : 0.650347888469696\n",
      "Epoch : 164 , loss : 0.6502656936645508\n",
      "Epoch : 165 , loss : 0.6501827836036682\n",
      "Epoch : 166 , loss : 0.6500979661941528\n",
      "Epoch : 167 , loss : 0.6500104665756226\n",
      "Epoch : 168 , loss : 0.6499208807945251\n",
      "Epoch : 169 , loss : 0.6498289108276367\n",
      "Epoch : 170 , loss : 0.6497358083724976\n",
      "Epoch : 171 , loss : 0.6496416926383972\n",
      "Epoch : 172 , loss : 0.6495464444160461\n",
      "Epoch : 173 , loss : 0.649450421333313\n",
      "Epoch : 174 , loss : 0.6493538618087769\n",
      "Epoch : 175 , loss : 0.6492577791213989\n",
      "Epoch : 176 , loss : 0.6491595506668091\n",
      "Epoch : 177 , loss : 0.649058997631073\n",
      "Epoch : 178 , loss : 0.6489557027816772\n",
      "Epoch : 179 , loss : 0.6488499045372009\n",
      "Epoch : 180 , loss : 0.6487413048744202\n",
      "Epoch : 181 , loss : 0.6486296653747559\n",
      "Epoch : 182 , loss : 0.6485152840614319\n",
      "Epoch : 183 , loss : 0.6483994126319885\n",
      "Epoch : 184 , loss : 0.6482808589935303\n",
      "Epoch : 185 , loss : 0.6481589674949646\n",
      "Epoch : 186 , loss : 0.6480348110198975\n",
      "Epoch : 187 , loss : 0.6479080319404602\n",
      "Epoch : 188 , loss : 0.6477780342102051\n",
      "Epoch : 189 , loss : 0.6476454138755798\n",
      "Epoch : 190 , loss : 0.647508978843689\n",
      "Epoch : 191 , loss : 0.6473690867424011\n",
      "Epoch : 192 , loss : 0.647225558757782\n",
      "Epoch : 193 , loss : 0.6470788717269897\n",
      "Epoch : 194 , loss : 0.6469300389289856\n",
      "Epoch : 195 , loss : 0.6467774510383606\n",
      "Epoch : 196 , loss : 0.6466218829154968\n",
      "Epoch : 197 , loss : 0.64646315574646\n",
      "Epoch : 198 , loss : 0.6463029980659485\n",
      "Epoch : 199 , loss : 0.6461389064788818\n",
      "Epoch : 200 , loss : 0.6459713578224182\n",
      "Epoch : 201 , loss : 0.6457991600036621\n",
      "Epoch : 202 , loss : 0.6456223130226135\n",
      "Epoch : 203 , loss : 0.6454402208328247\n",
      "Epoch : 204 , loss : 0.6452526450157166\n",
      "Epoch : 205 , loss : 0.6450600624084473\n",
      "Epoch : 206 , loss : 0.6448630094528198\n",
      "Epoch : 207 , loss : 0.6446619629859924\n",
      "Epoch : 208 , loss : 0.644456684589386\n",
      "Epoch : 209 , loss : 0.6442458033561707\n",
      "Epoch : 210 , loss : 0.6440308094024658\n",
      "Epoch : 211 , loss : 0.6438109278678894\n",
      "Epoch : 212 , loss : 0.643585741519928\n",
      "Epoch : 213 , loss : 0.6433541178703308\n",
      "Epoch : 214 , loss : 0.6431176662445068\n",
      "Epoch : 215 , loss : 0.6428762078285217\n",
      "Epoch : 216 , loss : 0.6426279544830322\n",
      "Epoch : 217 , loss : 0.6423736810684204\n",
      "Epoch : 218 , loss : 0.6421136260032654\n",
      "Epoch : 219 , loss : 0.6418472528457642\n",
      "Epoch : 220 , loss : 0.6415748596191406\n",
      "Epoch : 221 , loss : 0.6412943601608276\n",
      "Epoch : 222 , loss : 0.6410061120986938\n",
      "Epoch : 223 , loss : 0.6407100558280945\n",
      "Epoch : 224 , loss : 0.6404064297676086\n",
      "Epoch : 225 , loss : 0.6400942802429199\n",
      "Epoch : 226 , loss : 0.6397753357887268\n",
      "Epoch : 227 , loss : 0.6394497156143188\n",
      "Epoch : 228 , loss : 0.639116108417511\n",
      "Epoch : 229 , loss : 0.63877272605896\n",
      "Epoch : 230 , loss : 0.6384195685386658\n",
      "Epoch : 231 , loss : 0.6380583047866821\n",
      "Epoch : 232 , loss : 0.6376893520355225\n",
      "Epoch : 233 , loss : 0.6373132467269897\n",
      "Epoch : 234 , loss : 0.6369267106056213\n",
      "Epoch : 235 , loss : 0.6365309357643127\n",
      "Epoch : 236 , loss : 0.6361274719238281\n",
      "Epoch : 237 , loss : 0.6357125043869019\n",
      "Epoch : 238 , loss : 0.6352859735488892\n",
      "Epoch : 239 , loss : 0.6348503828048706\n",
      "Epoch : 240 , loss : 0.6344060301780701\n",
      "Epoch : 241 , loss : 0.6339482069015503\n",
      "Epoch : 242 , loss : 0.6334770321846008\n",
      "Epoch : 243 , loss : 0.6329917311668396\n",
      "Epoch : 244 , loss : 0.6324912309646606\n",
      "Epoch : 245 , loss : 0.6319757699966431\n",
      "Epoch : 246 , loss : 0.6314418315887451\n",
      "Epoch : 247 , loss : 0.6308911442756653\n",
      "Epoch : 248 , loss : 0.6303234696388245\n",
      "Epoch : 249 , loss : 0.6297381520271301\n",
      "Epoch : 250 , loss : 0.6291351914405823\n",
      "Epoch : 251 , loss : 0.6285134553909302\n",
      "Epoch : 252 , loss : 0.6278724074363708\n",
      "Epoch : 253 , loss : 0.6272143125534058\n",
      "Epoch : 254 , loss : 0.6265377402305603\n",
      "Epoch : 255 , loss : 0.6258398294448853\n",
      "Epoch : 256 , loss : 0.6251189708709717\n",
      "Epoch : 257 , loss : 0.624374508857727\n",
      "Epoch : 258 , loss : 0.6236065626144409\n",
      "Epoch : 259 , loss : 0.6228225231170654\n",
      "Epoch : 260 , loss : 0.6220173239707947\n",
      "Epoch : 261 , loss : 0.6211863160133362\n",
      "Epoch : 262 , loss : 0.620330274105072\n",
      "Epoch : 263 , loss : 0.6194466352462769\n",
      "Epoch : 264 , loss : 0.6185361742973328\n",
      "Epoch : 265 , loss : 0.6175931692123413\n",
      "Epoch : 266 , loss : 0.6166190505027771\n",
      "Epoch : 267 , loss : 0.6156182289123535\n",
      "Epoch : 268 , loss : 0.6145966649055481\n",
      "Epoch : 269 , loss : 0.6135438084602356\n",
      "Epoch : 270 , loss : 0.6124578714370728\n",
      "Epoch : 271 , loss : 0.6113387942314148\n",
      "Epoch : 272 , loss : 0.6101838946342468\n",
      "Epoch : 273 , loss : 0.6089886426925659\n",
      "Epoch : 274 , loss : 0.6077534556388855\n",
      "Epoch : 275 , loss : 0.6064811944961548\n",
      "Epoch : 276 , loss : 0.6051645278930664\n",
      "Epoch : 277 , loss : 0.6038053631782532\n",
      "Epoch : 278 , loss : 0.6024034023284912\n",
      "Epoch : 279 , loss : 0.600956380367279\n",
      "Epoch : 280 , loss : 0.5994694828987122\n",
      "Epoch : 281 , loss : 0.5979345440864563\n",
      "Epoch : 282 , loss : 0.5963502526283264\n",
      "Epoch : 283 , loss : 0.5947147607803345\n",
      "Epoch : 284 , loss : 0.5930235981941223\n",
      "Epoch : 285 , loss : 0.5912706255912781\n",
      "Epoch : 286 , loss : 0.5894659757614136\n",
      "Epoch : 287 , loss : 0.5876047611236572\n",
      "Epoch : 288 , loss : 0.5856832265853882\n",
      "Epoch : 289 , loss : 0.583703875541687\n",
      "Epoch : 290 , loss : 0.5816715955734253\n",
      "Epoch : 291 , loss : 0.5795787572860718\n",
      "Epoch : 292 , loss : 0.5774261951446533\n",
      "Epoch : 293 , loss : 0.5752119421958923\n",
      "Epoch : 294 , loss : 0.5729292035102844\n",
      "Epoch : 295 , loss : 0.5705742835998535\n",
      "Epoch : 296 , loss : 0.5681436061859131\n",
      "Epoch : 297 , loss : 0.5656505823135376\n",
      "Epoch : 298 , loss : 0.5630825757980347\n",
      "Epoch : 299 , loss : 0.5604335069656372\n",
      "Epoch : 300 , loss : 0.55770343542099\n",
      "Epoch : 301 , loss : 0.5548929572105408\n",
      "Epoch : 302 , loss : 0.5519891977310181\n",
      "Epoch : 303 , loss : 0.5490095615386963\n",
      "Epoch : 304 , loss : 0.5459561944007874\n",
      "Epoch : 305 , loss : 0.5428046584129333\n",
      "Epoch : 306 , loss : 0.5395690202713013\n",
      "Epoch : 307 , loss : 0.5362448692321777\n",
      "Epoch : 308 , loss : 0.5328372120857239\n",
      "Epoch : 309 , loss : 0.5293388366699219\n",
      "Epoch : 310 , loss : 0.5257357358932495\n",
      "Epoch : 311 , loss : 0.5220333933830261\n",
      "Epoch : 312 , loss : 0.5182235836982727\n",
      "Epoch : 313 , loss : 0.5143239498138428\n",
      "Epoch : 314 , loss : 0.5103211402893066\n",
      "Epoch : 315 , loss : 0.5062195062637329\n",
      "Epoch : 316 , loss : 0.5020316243171692\n",
      "Epoch : 317 , loss : 0.49775344133377075\n",
      "Epoch : 318 , loss : 0.4941960275173187\n",
      "Epoch : 319 , loss : 0.4897640645503998\n",
      "Epoch : 320 , loss : 0.48607149720191956\n",
      "Epoch : 321 , loss : 0.48161372542381287\n",
      "Epoch : 322 , loss : 0.47782063484191895\n",
      "Epoch : 323 , loss : 0.4732101261615753\n",
      "Epoch : 324 , loss : 0.46937310695648193\n",
      "Epoch : 325 , loss : 0.46465227007865906\n",
      "Epoch : 326 , loss : 0.46073734760284424\n",
      "Epoch : 327 , loss : 0.4561571776866913\n",
      "Epoch : 328 , loss : 0.45185279846191406\n",
      "Epoch : 329 , loss : 0.4477218687534332\n",
      "Epoch : 330 , loss : 0.4430277645587921\n",
      "Epoch : 331 , loss : 0.4390595853328705\n",
      "Epoch : 332 , loss : 0.43448594212532043\n",
      "Epoch : 333 , loss : 0.4298030734062195\n",
      "Epoch : 334 , loss : 0.42597031593322754\n",
      "Epoch : 335 , loss : 0.4213886559009552\n",
      "Epoch : 336 , loss : 0.4168010652065277\n",
      "Epoch : 337 , loss : 0.41284745931625366\n",
      "Epoch : 338 , loss : 0.4084171652793884\n",
      "Epoch : 339 , loss : 0.40382495522499084\n",
      "Epoch : 340 , loss : 0.3994251489639282\n",
      "Epoch : 341 , loss : 0.39556971192359924\n",
      "Epoch : 342 , loss : 0.39114803075790405\n",
      "Epoch : 343 , loss : 0.3867628574371338\n",
      "Epoch : 344 , loss : 0.382725328207016\n",
      "Epoch : 345 , loss : 0.3787767291069031\n",
      "Epoch : 346 , loss : 0.3744640350341797\n",
      "Epoch : 347 , loss : 0.3702372908592224\n",
      "Epoch : 348 , loss : 0.3661785423755646\n",
      "Epoch : 349 , loss : 0.36264821887016296\n",
      "Epoch : 350 , loss : 0.3585720956325531\n",
      "Epoch : 351 , loss : 0.3545529842376709\n",
      "Epoch : 352 , loss : 0.3506124019622803\n",
      "Epoch : 353 , loss : 0.3468213677406311\n",
      "Epoch : 354 , loss : 0.34348762035369873\n",
      "Epoch : 355 , loss : 0.33967307209968567\n",
      "Epoch : 356 , loss : 0.3359297513961792\n",
      "Epoch : 357 , loss : 0.33229851722717285\n",
      "Epoch : 358 , loss : 0.3287268280982971\n",
      "Epoch : 359 , loss : 0.3253866732120514\n",
      "Epoch : 360 , loss : 0.32225149869918823\n",
      "Epoch : 361 , loss : 0.3188183605670929\n",
      "Epoch : 362 , loss : 0.3153499960899353\n",
      "Epoch : 363 , loss : 0.312017560005188\n",
      "Epoch : 364 , loss : 0.30876854062080383\n",
      "Epoch : 365 , loss : 0.30558741092681885\n",
      "Epoch : 366 , loss : 0.3028833866119385\n",
      "Epoch : 367 , loss : 0.2999039590358734\n",
      "Epoch : 368 , loss : 0.29684963822364807\n",
      "Epoch : 369 , loss : 0.2938506305217743\n",
      "Epoch : 370 , loss : 0.29091426730155945\n",
      "Epoch : 371 , loss : 0.28806501626968384\n",
      "Epoch : 372 , loss : 0.28526943922042847\n",
      "Epoch : 373 , loss : 0.2826552391052246\n",
      "Epoch : 374 , loss : 0.28026387095451355\n",
      "Epoch : 375 , loss : 0.2775931656360626\n",
      "Epoch : 376 , loss : 0.274970680475235\n",
      "Epoch : 377 , loss : 0.2723994553089142\n",
      "Epoch : 378 , loss : 0.26990488171577454\n",
      "Epoch : 379 , loss : 0.2674773931503296\n",
      "Epoch : 380 , loss : 0.26509183645248413\n",
      "Epoch : 381 , loss : 0.26275792717933655\n",
      "Epoch : 382 , loss : 0.260814368724823\n",
      "Epoch : 383 , loss : 0.2585030794143677\n",
      "Epoch : 384 , loss : 0.2562306523323059\n",
      "Epoch : 385 , loss : 0.2539951503276825\n",
      "Epoch : 386 , loss : 0.25179779529571533\n",
      "Epoch : 387 , loss : 0.24963809549808502\n",
      "Epoch : 388 , loss : 0.24751374125480652\n",
      "Epoch : 389 , loss : 0.24541893601417542\n",
      "Epoch : 390 , loss : 0.24335415661334991\n",
      "Epoch : 391 , loss : 0.24131812155246735\n",
      "Epoch : 392 , loss : 0.23933526873588562\n",
      "Epoch : 393 , loss : 0.23758091032505035\n",
      "Epoch : 394 , loss : 0.23560190200805664\n",
      "Epoch : 395 , loss : 0.23365110158920288\n",
      "Epoch : 396 , loss : 0.2317296266555786\n",
      "Epoch : 397 , loss : 0.22983720898628235\n",
      "Epoch : 398 , loss : 0.22798950970172882\n",
      "Epoch : 399 , loss : 0.2261689305305481\n",
      "Epoch : 400 , loss : 0.22437913715839386\n",
      "Epoch : 401 , loss : 0.22261327505111694\n",
      "Epoch : 402 , loss : 0.22087347507476807\n",
      "Epoch : 403 , loss : 0.21915780007839203\n",
      "Epoch : 404 , loss : 0.21746647357940674\n",
      "Epoch : 405 , loss : 0.21580199897289276\n",
      "Epoch : 406 , loss : 0.21415653824806213\n",
      "Epoch : 407 , loss : 0.2126529961824417\n",
      "Epoch : 408 , loss : 0.21112878620624542\n",
      "Epoch : 409 , loss : 0.20951706171035767\n",
      "Epoch : 410 , loss : 0.20794998109340668\n",
      "Epoch : 411 , loss : 0.20639638602733612\n",
      "Epoch : 412 , loss : 0.20486211776733398\n",
      "Epoch : 413 , loss : 0.20335878431797028\n",
      "Epoch : 414 , loss : 0.201870396733284\n",
      "Epoch : 415 , loss : 0.20040233433246613\n",
      "Epoch : 416 , loss : 0.19896858930587769\n",
      "Epoch : 417 , loss : 0.1975516527891159\n",
      "Epoch : 418 , loss : 0.19617074728012085\n",
      "Epoch : 419 , loss : 0.19480471312999725\n",
      "Epoch : 420 , loss : 0.19346262514591217\n",
      "Epoch : 421 , loss : 0.1921369880437851\n",
      "Epoch : 422 , loss : 0.19083231687545776\n",
      "Epoch : 423 , loss : 0.18955323100090027\n",
      "Epoch : 424 , loss : 0.18829068541526794\n",
      "Epoch : 425 , loss : 0.18712656199932098\n",
      "Epoch : 426 , loss : 0.18596091866493225\n",
      "Epoch : 427 , loss : 0.18474018573760986\n",
      "Epoch : 428 , loss : 0.1835402250289917\n",
      "Epoch : 429 , loss : 0.18235215544700623\n",
      "Epoch : 430 , loss : 0.18118272721767426\n",
      "Epoch : 431 , loss : 0.18003325164318085\n",
      "Epoch : 432 , loss : 0.17888829112052917\n",
      "Epoch : 433 , loss : 0.17770080268383026\n",
      "Epoch : 434 , loss : 0.17653010785579681\n",
      "Epoch : 435 , loss : 0.1753753423690796\n",
      "Epoch : 436 , loss : 0.1742376983165741\n",
      "Epoch : 437 , loss : 0.17311687767505646\n",
      "Epoch : 438 , loss : 0.17201228439807892\n",
      "Epoch : 439 , loss : 0.1709211766719818\n",
      "Epoch : 440 , loss : 0.16985219717025757\n",
      "Epoch : 441 , loss : 0.16881516575813293\n",
      "Epoch : 442 , loss : 0.16779009997844696\n",
      "Epoch : 443 , loss : 0.16678161919116974\n",
      "Epoch : 444 , loss : 0.1657862514257431\n",
      "Epoch : 445 , loss : 0.16480199992656708\n",
      "Epoch : 446 , loss : 0.1638304889202118\n",
      "Epoch : 447 , loss : 0.1628825068473816\n",
      "Epoch : 448 , loss : 0.16203433275222778\n",
      "Epoch : 449 , loss : 0.1610926389694214\n",
      "Epoch : 450 , loss : 0.16016057133674622\n",
      "Epoch : 451 , loss : 0.1592443734407425\n",
      "Epoch : 452 , loss : 0.1583411544561386\n",
      "Epoch : 453 , loss : 0.15744663774967194\n",
      "Epoch : 454 , loss : 0.15656700730323792\n",
      "Epoch : 455 , loss : 0.15570935606956482\n",
      "Epoch : 456 , loss : 0.15485647320747375\n",
      "Epoch : 457 , loss : 0.15402032434940338\n",
      "Epoch : 458 , loss : 0.15319234132766724\n",
      "Epoch : 459 , loss : 0.15237469971179962\n",
      "Epoch : 460 , loss : 0.15157078206539154\n",
      "Epoch : 461 , loss : 0.1507723331451416\n",
      "Epoch : 462 , loss : 0.14999032020568848\n",
      "Epoch : 463 , loss : 0.14921045303344727\n",
      "Epoch : 464 , loss : 0.14844699203968048\n",
      "Epoch : 465 , loss : 0.14768682420253754\n",
      "Epoch : 466 , loss : 0.14693963527679443\n",
      "Epoch : 467 , loss : 0.14619998633861542\n",
      "Epoch : 468 , loss : 0.14547251164913177\n",
      "Epoch : 469 , loss : 0.1447511613368988\n",
      "Epoch : 470 , loss : 0.14404930174350739\n",
      "Epoch : 471 , loss : 0.14342762529850006\n",
      "Epoch : 472 , loss : 0.1427369862794876\n",
      "Epoch : 473 , loss : 0.14205127954483032\n",
      "Epoch : 474 , loss : 0.1413774937391281\n",
      "Epoch : 475 , loss : 0.14071142673492432\n",
      "Epoch : 476 , loss : 0.1400509774684906\n",
      "Epoch : 477 , loss : 0.1394021362066269\n",
      "Epoch : 478 , loss : 0.1387580782175064\n",
      "Epoch : 479 , loss : 0.1381203532218933\n",
      "Epoch : 480 , loss : 0.13749535381793976\n",
      "Epoch : 481 , loss : 0.13687145709991455\n",
      "Epoch : 482 , loss : 0.13625642657279968\n",
      "Epoch : 483 , loss : 0.13565129041671753\n",
      "Epoch : 484 , loss : 0.13504713773727417\n",
      "Epoch : 485 , loss : 0.13445614278316498\n",
      "Epoch : 486 , loss : 0.1338682919740677\n",
      "Epoch : 487 , loss : 0.1332860291004181\n",
      "Epoch : 488 , loss : 0.13271456956863403\n",
      "Epoch : 489 , loss : 0.1321428120136261\n",
      "Epoch : 490 , loss : 0.13158531486988068\n",
      "Epoch : 491 , loss : 0.13102580606937408\n",
      "Epoch : 492 , loss : 0.13048100471496582\n",
      "Epoch : 493 , loss : 0.12993423640727997\n",
      "Epoch : 494 , loss : 0.12942904233932495\n",
      "Epoch : 495 , loss : 0.12892793118953705\n",
      "Epoch : 496 , loss : 0.1283991038799286\n",
      "Epoch : 497 , loss : 0.12787461280822754\n",
      "Epoch : 498 , loss : 0.127354696393013\n",
      "Epoch : 499 , loss : 0.1268439143896103\n",
      "Epoch : 500 , loss : 0.12633277475833893\n",
      "Epoch : 501 , loss : 0.1258334517478943\n",
      "Epoch : 502 , loss : 0.12533356249332428\n",
      "Epoch : 503 , loss : 0.12484176456928253\n",
      "Epoch : 504 , loss : 0.12435513734817505\n",
      "Epoch : 505 , loss : 0.12387023866176605\n",
      "Epoch : 506 , loss : 0.12339609116315842\n",
      "Epoch : 507 , loss : 0.12291954457759857\n",
      "Epoch : 508 , loss : 0.12245403230190277\n",
      "Epoch : 509 , loss : 0.12198878079652786\n",
      "Epoch : 510 , loss : 0.12153323739767075\n",
      "Epoch : 511 , loss : 0.12108452618122101\n",
      "Epoch : 512 , loss : 0.12063882499933243\n",
      "Epoch : 513 , loss : 0.12020060420036316\n",
      "Epoch : 514 , loss : 0.11976132541894913\n",
      "Epoch : 515 , loss : 0.11933311074972153\n",
      "Epoch : 516 , loss : 0.11890196800231934\n",
      "Epoch : 517 , loss : 0.11848625540733337\n",
      "Epoch : 518 , loss : 0.11806468665599823\n",
      "Epoch : 519 , loss : 0.1176573857665062\n",
      "Epoch : 520 , loss : 0.11729349195957184\n",
      "Epoch : 521 , loss : 0.11687792092561722\n",
      "Epoch : 522 , loss : 0.11647514253854752\n",
      "Epoch : 523 , loss : 0.11607508361339569\n",
      "Epoch : 524 , loss : 0.11567514389753342\n",
      "Epoch : 525 , loss : 0.1152835488319397\n",
      "Epoch : 526 , loss : 0.11489608883857727\n",
      "Epoch : 527 , loss : 0.11450690031051636\n",
      "Epoch : 528 , loss : 0.11412541568279266\n",
      "Epoch : 529 , loss : 0.11374688893556595\n",
      "Epoch : 530 , loss : 0.11336944997310638\n",
      "Epoch : 531 , loss : 0.11299655586481094\n",
      "Epoch : 532 , loss : 0.11262904852628708\n",
      "Epoch : 533 , loss : 0.11226554960012436\n",
      "Epoch : 534 , loss : 0.11189936846494675\n",
      "Epoch : 535 , loss : 0.11154159903526306\n",
      "Epoch : 536 , loss : 0.11118367314338684\n",
      "Epoch : 537 , loss : 0.11082904040813446\n",
      "Epoch : 538 , loss : 0.11047931760549545\n",
      "Epoch : 539 , loss : 0.11013291776180267\n",
      "Epoch : 540 , loss : 0.10978780686855316\n",
      "Epoch : 541 , loss : 0.10944525897502899\n",
      "Epoch : 542 , loss : 0.10911010950803757\n",
      "Epoch : 543 , loss : 0.10877078026533127\n",
      "Epoch : 544 , loss : 0.10843878984451294\n",
      "Epoch : 545 , loss : 0.1081094816327095\n",
      "Epoch : 546 , loss : 0.10777970403432846\n",
      "Epoch : 547 , loss : 0.10745429247617722\n",
      "Epoch : 548 , loss : 0.1071329191327095\n",
      "Epoch : 549 , loss : 0.10681085288524628\n",
      "Epoch : 550 , loss : 0.10649363696575165\n",
      "Epoch : 551 , loss : 0.10618048161268234\n",
      "Epoch : 552 , loss : 0.10587084293365479\n",
      "Epoch : 553 , loss : 0.10558099299669266\n",
      "Epoch : 554 , loss : 0.10527648031711578\n",
      "Epoch : 555 , loss : 0.10496775060892105\n",
      "Epoch : 556 , loss : 0.10466410964727402\n",
      "Epoch : 557 , loss : 0.10436862707138062\n",
      "Epoch : 558 , loss : 0.10406409204006195\n",
      "Epoch : 559 , loss : 0.10378358513116837\n",
      "Epoch : 560 , loss : 0.10347755253314972\n",
      "Epoch : 561 , loss : 0.10318301618099213\n",
      "Epoch : 562 , loss : 0.1028977707028389\n",
      "Epoch : 563 , loss : 0.10261797904968262\n",
      "Epoch : 564 , loss : 0.10232243686914444\n",
      "Epoch : 565 , loss : 0.10204152762889862\n",
      "Epoch : 566 , loss : 0.10176106542348862\n",
      "Epoch : 567 , loss : 0.10147802531719208\n",
      "Epoch : 568 , loss : 0.10120492428541183\n",
      "Epoch : 569 , loss : 0.10093823820352554\n",
      "Epoch : 570 , loss : 0.10065387934446335\n",
      "Epoch : 571 , loss : 0.10038711875677109\n",
      "Epoch : 572 , loss : 0.10011596977710724\n",
      "Epoch : 573 , loss : 0.09984593838453293\n",
      "Epoch : 574 , loss : 0.09958516061306\n",
      "Epoch : 575 , loss : 0.09932206571102142\n",
      "Epoch : 576 , loss : 0.0990670695900917\n",
      "Epoch : 577 , loss : 0.09879730641841888\n",
      "Epoch : 578 , loss : 0.09854461252689362\n",
      "Epoch : 579 , loss : 0.09828726202249527\n",
      "Epoch : 580 , loss : 0.09802975505590439\n",
      "Epoch : 581 , loss : 0.09777813404798508\n",
      "Epoch : 582 , loss : 0.09752756357192993\n",
      "Epoch : 583 , loss : 0.09728286415338516\n",
      "Epoch : 584 , loss : 0.09703453630208969\n",
      "Epoch : 585 , loss : 0.09678751230239868\n",
      "Epoch : 586 , loss : 0.09653928130865097\n",
      "Epoch : 587 , loss : 0.0962989404797554\n",
      "Epoch : 588 , loss : 0.09605588763952255\n",
      "Epoch : 589 , loss : 0.09581701457500458\n",
      "Epoch : 590 , loss : 0.09557880461215973\n",
      "Epoch : 591 , loss : 0.09534771740436554\n",
      "Epoch : 592 , loss : 0.0951109305024147\n",
      "Epoch : 593 , loss : 0.09487807005643845\n",
      "Epoch : 594 , loss : 0.09464751929044724\n",
      "Epoch : 595 , loss : 0.09443588554859161\n",
      "Epoch : 596 , loss : 0.09420034289360046\n",
      "Epoch : 597 , loss : 0.09396992623806\n",
      "Epoch : 598 , loss : 0.09375276416540146\n",
      "Epoch : 599 , loss : 0.09351891279220581\n",
      "Epoch : 600 , loss : 0.09330257028341293\n",
      "Epoch : 601 , loss : 0.09307347238063812\n",
      "Epoch : 602 , loss : 0.0928528755903244\n",
      "Epoch : 603 , loss : 0.09263920038938522\n",
      "Epoch : 604 , loss : 0.09241487830877304\n",
      "Epoch : 605 , loss : 0.0921979695558548\n",
      "Epoch : 606 , loss : 0.09198852628469467\n",
      "Epoch : 607 , loss : 0.09176891297101974\n",
      "Epoch : 608 , loss : 0.09155508875846863\n",
      "Epoch : 609 , loss : 0.09135253727436066\n",
      "Epoch : 610 , loss : 0.09113249182701111\n",
      "Epoch : 611 , loss : 0.09092912822961807\n",
      "Epoch : 612 , loss : 0.09072362631559372\n",
      "Epoch : 613 , loss : 0.09051472693681717\n",
      "Epoch : 614 , loss : 0.0903068482875824\n",
      "Epoch : 615 , loss : 0.0901074931025505\n",
      "Epoch : 616 , loss : 0.08990786224603653\n",
      "Epoch : 617 , loss : 0.08970382809638977\n",
      "Epoch : 618 , loss : 0.08950002491474152\n",
      "Epoch : 619 , loss : 0.08930501341819763\n",
      "Epoch : 620 , loss : 0.08911057561635971\n",
      "Epoch : 621 , loss : 0.08891165256500244\n",
      "Epoch : 622 , loss : 0.08871346712112427\n",
      "Epoch : 623 , loss : 0.08852072805166245\n",
      "Epoch : 624 , loss : 0.08833041042089462\n",
      "Epoch : 625 , loss : 0.08814137428998947\n",
      "Epoch : 626 , loss : 0.08794611692428589\n",
      "Epoch : 627 , loss : 0.0877571552991867\n",
      "Epoch : 628 , loss : 0.08757171779870987\n",
      "Epoch : 629 , loss : 0.08738616853952408\n",
      "Epoch : 630 , loss : 0.08719800412654877\n",
      "Epoch : 631 , loss : 0.08701183646917343\n",
      "Epoch : 632 , loss : 0.08682547509670258\n",
      "Epoch : 633 , loss : 0.08664489537477493\n",
      "Epoch : 634 , loss : 0.08646184951066971\n",
      "Epoch : 635 , loss : 0.08628138154745102\n",
      "Epoch : 636 , loss : 0.08610275387763977\n",
      "Epoch : 637 , loss : 0.08592355251312256\n",
      "Epoch : 638 , loss : 0.08574488759040833\n",
      "Epoch : 639 , loss : 0.08556858450174332\n",
      "Epoch : 640 , loss : 0.08539599925279617\n",
      "Epoch : 641 , loss : 0.08521884679794312\n",
      "Epoch : 642 , loss : 0.0850539430975914\n",
      "Epoch : 643 , loss : 0.08487781137228012\n",
      "Epoch : 644 , loss : 0.08470673114061356\n",
      "Epoch : 645 , loss : 0.08454351127147675\n",
      "Epoch : 646 , loss : 0.0843692496418953\n",
      "Epoch : 647 , loss : 0.08420167118310928\n",
      "Epoch : 648 , loss : 0.0840388610959053\n",
      "Epoch : 649 , loss : 0.08386888355016708\n",
      "Epoch : 650 , loss : 0.08370476216077805\n",
      "Epoch : 651 , loss : 0.0835438072681427\n",
      "Epoch : 652 , loss : 0.08337729424238205\n",
      "Epoch : 653 , loss : 0.08321304619312286\n",
      "Epoch : 654 , loss : 0.08305079489946365\n",
      "Epoch : 655 , loss : 0.0828918069601059\n",
      "Epoch : 656 , loss : 0.08273079246282578\n",
      "Epoch : 657 , loss : 0.08257065713405609\n",
      "Epoch : 658 , loss : 0.08241424709558487\n",
      "Epoch : 659 , loss : 0.0822577029466629\n",
      "Epoch : 660 , loss : 0.08210241794586182\n",
      "Epoch : 661 , loss : 0.08194444328546524\n",
      "Epoch : 662 , loss : 0.08178906887769699\n",
      "Epoch : 663 , loss : 0.08163190633058548\n",
      "Epoch : 664 , loss : 0.08148075640201569\n",
      "Epoch : 665 , loss : 0.08132711052894592\n",
      "Epoch : 666 , loss : 0.08117736876010895\n",
      "Epoch : 667 , loss : 0.081025630235672\n",
      "Epoch : 668 , loss : 0.08087664097547531\n",
      "Epoch : 669 , loss : 0.08073257654905319\n",
      "Epoch : 670 , loss : 0.08058890700340271\n",
      "Epoch : 671 , loss : 0.08044929057359695\n",
      "Epoch : 672 , loss : 0.08030110597610474\n",
      "Epoch : 673 , loss : 0.08015651255846024\n",
      "Epoch : 674 , loss : 0.08001039177179337\n",
      "Epoch : 675 , loss : 0.07986802607774734\n",
      "Epoch : 676 , loss : 0.07972361892461777\n",
      "Epoch : 677 , loss : 0.07958349585533142\n",
      "Epoch : 678 , loss : 0.07944349944591522\n",
      "Epoch : 679 , loss : 0.07930576801300049\n",
      "Epoch : 680 , loss : 0.07916491478681564\n",
      "Epoch : 681 , loss : 0.07902725785970688\n",
      "Epoch : 682 , loss : 0.07888906449079514\n",
      "Epoch : 683 , loss : 0.07875235378742218\n",
      "Epoch : 684 , loss : 0.0786164328455925\n",
      "Epoch : 685 , loss : 0.0784783661365509\n",
      "Epoch : 686 , loss : 0.07834918051958084\n",
      "Epoch : 687 , loss : 0.07821415364742279\n",
      "Epoch : 688 , loss : 0.07807662338018417\n",
      "Epoch : 689 , loss : 0.07794275134801865\n",
      "Epoch : 690 , loss : 0.07780949026346207\n",
      "Epoch : 691 , loss : 0.07768184691667557\n",
      "Epoch : 692 , loss : 0.07754568755626678\n",
      "Epoch : 693 , loss : 0.0774179995059967\n",
      "Epoch : 694 , loss : 0.07728780061006546\n",
      "Epoch : 695 , loss : 0.07715702801942825\n",
      "Epoch : 696 , loss : 0.07702863961458206\n",
      "Epoch : 697 , loss : 0.07689861953258514\n",
      "Epoch : 698 , loss : 0.07677307724952698\n",
      "Epoch : 699 , loss : 0.07664526998996735\n",
      "Epoch : 700 , loss : 0.07651527971029282\n",
      "Epoch : 701 , loss : 0.07639355212450027\n",
      "Epoch : 702 , loss : 0.0762704387307167\n",
      "Epoch : 703 , loss : 0.07614254206418991\n",
      "Epoch : 704 , loss : 0.07601527869701385\n",
      "Epoch : 705 , loss : 0.07589422911405563\n",
      "Epoch : 706 , loss : 0.07576809078454971\n",
      "Epoch : 707 , loss : 0.07564596831798553\n",
      "Epoch : 708 , loss : 0.07552304863929749\n",
      "Epoch : 709 , loss : 0.07540124654769897\n",
      "Epoch : 710 , loss : 0.07528147101402283\n",
      "Epoch : 711 , loss : 0.07515661418437958\n",
      "Epoch : 712 , loss : 0.07503911852836609\n",
      "Epoch : 713 , loss : 0.07491886615753174\n",
      "Epoch : 714 , loss : 0.07479918003082275\n",
      "Epoch : 715 , loss : 0.07468036562204361\n",
      "Epoch : 716 , loss : 0.07455843687057495\n",
      "Epoch : 717 , loss : 0.07445071637630463\n",
      "Epoch : 718 , loss : 0.07432540506124496\n",
      "Epoch : 719 , loss : 0.07420599460601807\n",
      "Epoch : 720 , loss : 0.07409481704235077\n",
      "Epoch : 721 , loss : 0.07397837936878204\n",
      "Epoch : 722 , loss : 0.07385910302400589\n",
      "Epoch : 723 , loss : 0.07374347746372223\n",
      "Epoch : 724 , loss : 0.07362593710422516\n",
      "Epoch : 725 , loss : 0.07351893931627274\n",
      "Epoch : 726 , loss : 0.07339806854724884\n",
      "Epoch : 727 , loss : 0.07328594475984573\n",
      "Epoch : 728 , loss : 0.07318009436130524\n",
      "Epoch : 729 , loss : 0.0730602964758873\n",
      "Epoch : 730 , loss : 0.07294883579015732\n",
      "Epoch : 731 , loss : 0.07284197211265564\n",
      "Epoch : 732 , loss : 0.07272637635469437\n",
      "Epoch : 733 , loss : 0.07261374592781067\n",
      "Epoch : 734 , loss : 0.0725022405385971\n",
      "Epoch : 735 , loss : 0.07239232957363129\n",
      "Epoch : 736 , loss : 0.07228555530309677\n",
      "Epoch : 737 , loss : 0.07217422872781754\n",
      "Epoch : 738 , loss : 0.07206517457962036\n",
      "Epoch : 739 , loss : 0.07195691764354706\n",
      "Epoch : 740 , loss : 0.07185021787881851\n",
      "Epoch : 741 , loss : 0.07173772901296616\n",
      "Epoch : 742 , loss : 0.07163457572460175\n",
      "Epoch : 743 , loss : 0.0715273767709732\n",
      "Epoch : 744 , loss : 0.07141988724470139\n",
      "Epoch : 745 , loss : 0.0713140144944191\n",
      "Epoch : 746 , loss : 0.07121274620294571\n",
      "Epoch : 747 , loss : 0.07110186666250229\n",
      "Epoch : 748 , loss : 0.07099851965904236\n",
      "Epoch : 749 , loss : 0.0708906501531601\n",
      "Epoch : 750 , loss : 0.07079548388719559\n",
      "Epoch : 751 , loss : 0.07068479061126709\n",
      "Epoch : 752 , loss : 0.07057883590459824\n",
      "Epoch : 753 , loss : 0.07047726213932037\n",
      "Epoch : 754 , loss : 0.07037626951932907\n",
      "Epoch : 755 , loss : 0.07027273625135422\n",
      "Epoch : 756 , loss : 0.07017000019550323\n",
      "Epoch : 757 , loss : 0.07007673382759094\n",
      "Epoch : 758 , loss : 0.06996981799602509\n",
      "Epoch : 759 , loss : 0.0698673352599144\n",
      "Epoch : 760 , loss : 0.06977397948503494\n",
      "Epoch : 761 , loss : 0.06966979801654816\n",
      "Epoch : 762 , loss : 0.06956727057695389\n",
      "Epoch : 763 , loss : 0.069468654692173\n",
      "Epoch : 764 , loss : 0.06936917454004288\n",
      "Epoch : 765 , loss : 0.06927946954965591\n",
      "Epoch : 766 , loss : 0.06917434185743332\n",
      "Epoch : 767 , loss : 0.06907466799020767\n",
      "Epoch : 768 , loss : 0.0689796432852745\n",
      "Epoch : 769 , loss : 0.06888038665056229\n",
      "Epoch : 770 , loss : 0.06878255307674408\n",
      "Epoch : 771 , loss : 0.06868777424097061\n",
      "Epoch : 772 , loss : 0.06859148293733597\n",
      "Epoch : 773 , loss : 0.06849335134029388\n",
      "Epoch : 774 , loss : 0.06839747726917267\n",
      "Epoch : 775 , loss : 0.06830835342407227\n",
      "Epoch : 776 , loss : 0.06820913404226303\n",
      "Epoch : 777 , loss : 0.06811156123876572\n",
      "Epoch : 778 , loss : 0.06802112609148026\n",
      "Epoch : 779 , loss : 0.06792905926704407\n",
      "Epoch : 780 , loss : 0.06782925873994827\n",
      "Epoch : 781 , loss : 0.06773755699396133\n",
      "Epoch : 782 , loss : 0.0676424503326416\n",
      "Epoch : 783 , loss : 0.06755093485116959\n",
      "Epoch : 784 , loss : 0.06746108829975128\n",
      "Epoch : 785 , loss : 0.06736668199300766\n",
      "Epoch : 786 , loss : 0.06727410852909088\n",
      "Epoch : 787 , loss : 0.06718473881483078\n",
      "Epoch : 788 , loss : 0.06709347665309906\n",
      "Epoch : 789 , loss : 0.06700063496828079\n",
      "Epoch : 790 , loss : 0.06691041588783264\n",
      "Epoch : 791 , loss : 0.0668259784579277\n",
      "Epoch : 792 , loss : 0.06673043221235275\n",
      "Epoch : 793 , loss : 0.0666414350271225\n",
      "Epoch : 794 , loss : 0.06654998660087585\n",
      "Epoch : 795 , loss : 0.06646054238080978\n",
      "Epoch : 796 , loss : 0.06637532263994217\n",
      "Epoch : 797 , loss : 0.0662841722369194\n",
      "Epoch : 798 , loss : 0.06619551032781601\n",
      "Epoch : 799 , loss : 0.06610811501741409\n",
      "Epoch : 800 , loss : 0.06601844727993011\n",
      "Epoch : 801 , loss : 0.06593652814626694\n",
      "Epoch : 802 , loss : 0.06584601104259491\n",
      "Epoch : 803 , loss : 0.06575657427310944\n",
      "Epoch : 804 , loss : 0.06567774713039398\n",
      "Epoch : 805 , loss : 0.0655876025557518\n",
      "Epoch : 806 , loss : 0.0654972568154335\n",
      "Epoch : 807 , loss : 0.06541214138269424\n",
      "Epoch : 808 , loss : 0.06532853841781616\n",
      "Epoch : 809 , loss : 0.06524695456027985\n",
      "Epoch : 810 , loss : 0.06515690684318542\n",
      "Epoch : 811 , loss : 0.06507209688425064\n",
      "Epoch : 812 , loss : 0.06498541682958603\n",
      "Epoch : 813 , loss : 0.06490866094827652\n",
      "Epoch : 814 , loss : 0.06481978297233582\n",
      "Epoch : 815 , loss : 0.06473289430141449\n",
      "Epoch : 816 , loss : 0.06465443223714828\n",
      "Epoch : 817 , loss : 0.06456755101680756\n",
      "Epoch : 818 , loss : 0.06448852270841599\n",
      "Epoch : 819 , loss : 0.06440509110689163\n",
      "Epoch : 820 , loss : 0.06431831419467926\n",
      "Epoch : 821 , loss : 0.06424501538276672\n",
      "Epoch : 822 , loss : 0.06415964663028717\n",
      "Epoch : 823 , loss : 0.06407323479652405\n",
      "Epoch : 824 , loss : 0.06399167329072952\n",
      "Epoch : 825 , loss : 0.06391336023807526\n",
      "Epoch : 826 , loss : 0.06383189558982849\n",
      "Epoch : 827 , loss : 0.06375095248222351\n",
      "Epoch : 828 , loss : 0.06367045640945435\n",
      "Epoch : 829 , loss : 0.06358662992715836\n",
      "Epoch : 830 , loss : 0.06351114809513092\n",
      "Epoch : 831 , loss : 0.06343114376068115\n",
      "Epoch : 832 , loss : 0.06334799528121948\n",
      "Epoch : 833 , loss : 0.06327175348997116\n",
      "Epoch : 834 , loss : 0.06319649517536163\n",
      "Epoch : 835 , loss : 0.06311202049255371\n",
      "Epoch : 836 , loss : 0.06303463876247406\n",
      "Epoch : 837 , loss : 0.06295402348041534\n",
      "Epoch : 838 , loss : 0.06287937611341476\n",
      "Epoch : 839 , loss : 0.06279858201742172\n",
      "Epoch : 840 , loss : 0.0627216324210167\n",
      "Epoch : 841 , loss : 0.06264325976371765\n",
      "Epoch : 842 , loss : 0.06256494671106339\n",
      "Epoch : 843 , loss : 0.06249144673347473\n",
      "Epoch : 844 , loss : 0.06241222470998764\n",
      "Epoch : 845 , loss : 0.062334369868040085\n",
      "Epoch : 846 , loss : 0.06225879490375519\n",
      "Epoch : 847 , loss : 0.06218394637107849\n",
      "Epoch : 848 , loss : 0.062105778604745865\n",
      "Epoch : 849 , loss : 0.06203100457787514\n",
      "Epoch : 850 , loss : 0.0619530975818634\n",
      "Epoch : 851 , loss : 0.06187933683395386\n",
      "Epoch : 852 , loss : 0.061806146055459976\n",
      "Epoch : 853 , loss : 0.061728961765766144\n",
      "Epoch : 854 , loss : 0.061651717871427536\n",
      "Epoch : 855 , loss : 0.06158553808927536\n",
      "Epoch : 856 , loss : 0.06150580570101738\n",
      "Epoch : 857 , loss : 0.06142808124423027\n",
      "Epoch : 858 , loss : 0.06135793775320053\n",
      "Epoch : 859 , loss : 0.06128235161304474\n",
      "Epoch : 860 , loss : 0.06120928004384041\n",
      "Epoch : 861 , loss : 0.06113731116056442\n",
      "Epoch : 862 , loss : 0.06106089800596237\n",
      "Epoch : 863 , loss : 0.06099134311079979\n",
      "Epoch : 864 , loss : 0.060918327420949936\n",
      "Epoch : 865 , loss : 0.060842037200927734\n",
      "Epoch : 866 , loss : 0.06076788157224655\n",
      "Epoch : 867 , loss : 0.060700833797454834\n",
      "Epoch : 868 , loss : 0.060626283288002014\n",
      "Epoch : 869 , loss : 0.06055327132344246\n",
      "Epoch : 870 , loss : 0.06048310175538063\n",
      "Epoch : 871 , loss : 0.06041017547249794\n",
      "Epoch : 872 , loss : 0.06034155189990997\n",
      "Epoch : 873 , loss : 0.06027054041624069\n",
      "Epoch : 874 , loss : 0.060194920748472214\n",
      "Epoch : 875 , loss : 0.06012468412518501\n",
      "Epoch : 876 , loss : 0.06005750969052315\n",
      "Epoch : 877 , loss : 0.05998384952545166\n",
      "Epoch : 878 , loss : 0.05991430580615997\n",
      "Epoch : 879 , loss : 0.05984479933977127\n",
      "Epoch : 880 , loss : 0.05977756530046463\n",
      "Epoch : 881 , loss : 0.059707097709178925\n",
      "Epoch : 882 , loss : 0.05963622406125069\n",
      "Epoch : 883 , loss : 0.05956268683075905\n",
      "Epoch : 884 , loss : 0.05949944257736206\n",
      "Epoch : 885 , loss : 0.0594278946518898\n",
      "Epoch : 886 , loss : 0.05935778468847275\n",
      "Epoch : 887 , loss : 0.059289440512657166\n",
      "Epoch : 888 , loss : 0.0592220276594162\n",
      "Epoch : 889 , loss : 0.059156253933906555\n",
      "Epoch : 890 , loss : 0.05908697471022606\n",
      "Epoch : 891 , loss : 0.05901510640978813\n",
      "Epoch : 892 , loss : 0.058948319405317307\n",
      "Epoch : 893 , loss : 0.05888316407799721\n",
      "Epoch : 894 , loss : 0.05881159380078316\n",
      "Epoch : 895 , loss : 0.05874672904610634\n",
      "Epoch : 896 , loss : 0.05867893993854523\n",
      "Epoch : 897 , loss : 0.058612484484910965\n",
      "Epoch : 898 , loss : 0.058544524013996124\n",
      "Epoch : 899 , loss : 0.05847766250371933\n",
      "Epoch : 900 , loss : 0.058411382138729095\n",
      "Epoch : 901 , loss : 0.0583491288125515\n",
      "Epoch : 902 , loss : 0.05827910080552101\n",
      "Epoch : 903 , loss : 0.058211080729961395\n",
      "Epoch : 904 , loss : 0.05814738944172859\n",
      "Epoch : 905 , loss : 0.05808275565505028\n",
      "Epoch : 906 , loss : 0.05801521614193916\n",
      "Epoch : 907 , loss : 0.05794919654726982\n",
      "Epoch : 908 , loss : 0.05788261815905571\n",
      "Epoch : 909 , loss : 0.057823993265628815\n",
      "Epoch : 910 , loss : 0.057756297290325165\n",
      "Epoch : 911 , loss : 0.057686932384967804\n",
      "Epoch : 912 , loss : 0.05762547254562378\n",
      "Epoch : 913 , loss : 0.05756229907274246\n",
      "Epoch : 914 , loss : 0.057495325803756714\n",
      "Epoch : 915 , loss : 0.05743076652288437\n",
      "Epoch : 916 , loss : 0.05736762285232544\n",
      "Epoch : 917 , loss : 0.05730558931827545\n",
      "Epoch : 918 , loss : 0.05724026635289192\n",
      "Epoch : 919 , loss : 0.057176779955625534\n",
      "Epoch : 920 , loss : 0.05711233615875244\n",
      "Epoch : 921 , loss : 0.05705481022596359\n",
      "Epoch : 922 , loss : 0.05698925629258156\n",
      "Epoch : 923 , loss : 0.056923042982816696\n",
      "Epoch : 924 , loss : 0.056860730051994324\n",
      "Epoch : 925 , loss : 0.05680075287818909\n",
      "Epoch : 926 , loss : 0.05673699826002121\n",
      "Epoch : 927 , loss : 0.05667572468519211\n",
      "Epoch : 928 , loss : 0.05661076679825783\n",
      "Epoch : 929 , loss : 0.056549813598394394\n",
      "Epoch : 930 , loss : 0.0564911775290966\n",
      "Epoch : 931 , loss : 0.05642612650990486\n",
      "Epoch : 932 , loss : 0.05636390671133995\n",
      "Epoch : 933 , loss : 0.05630503222346306\n",
      "Epoch : 934 , loss : 0.05624306946992874\n",
      "Epoch : 935 , loss : 0.05618339776992798\n",
      "Epoch : 936 , loss : 0.056118447333574295\n",
      "Epoch : 937 , loss : 0.05605858564376831\n",
      "Epoch : 938 , loss : 0.056000687181949615\n",
      "Epoch : 939 , loss : 0.05593849718570709\n",
      "Epoch : 940 , loss : 0.05587896332144737\n",
      "Epoch : 941 , loss : 0.055814336985349655\n",
      "Epoch : 942 , loss : 0.055756352841854095\n",
      "Epoch : 943 , loss : 0.055699072778224945\n",
      "Epoch : 944 , loss : 0.05563618242740631\n",
      "Epoch : 945 , loss : 0.055580709129571915\n",
      "Epoch : 946 , loss : 0.05551474168896675\n",
      "Epoch : 947 , loss : 0.05545588210225105\n",
      "Epoch : 948 , loss : 0.055401191115379333\n",
      "Epoch : 949 , loss : 0.055338069796562195\n",
      "Epoch : 950 , loss : 0.05528165027499199\n",
      "Epoch : 951 , loss : 0.05521899461746216\n",
      "Epoch : 952 , loss : 0.05516551062464714\n",
      "Epoch : 953 , loss : 0.05510234460234642\n",
      "Epoch : 954 , loss : 0.055045921355485916\n",
      "Epoch : 955 , loss : 0.05498487502336502\n",
      "Epoch : 956 , loss : 0.0549284853041172\n",
      "Epoch : 957 , loss : 0.0548725388944149\n",
      "Epoch : 958 , loss : 0.054812055081129074\n",
      "Epoch : 959 , loss : 0.05475394427776337\n",
      "Epoch : 960 , loss : 0.05469182878732681\n",
      "Epoch : 961 , loss : 0.054639969021081924\n",
      "Epoch : 962 , loss : 0.054581787437200546\n",
      "Epoch : 963 , loss : 0.05452205240726471\n",
      "Epoch : 964 , loss : 0.0544670931994915\n",
      "Epoch : 965 , loss : 0.05440519005060196\n",
      "Epoch : 966 , loss : 0.05435477942228317\n",
      "Epoch : 967 , loss : 0.054294560104608536\n",
      "Epoch : 968 , loss : 0.054236557334661484\n",
      "Epoch : 969 , loss : 0.054178640246391296\n",
      "Epoch : 970 , loss : 0.05412396416068077\n",
      "Epoch : 971 , loss : 0.05406927689909935\n",
      "Epoch : 972 , loss : 0.05401024967432022\n",
      "Epoch : 973 , loss : 0.053956449031829834\n",
      "Epoch : 974 , loss : 0.053896281868219376\n",
      "Epoch : 975 , loss : 0.053844209760427475\n",
      "Epoch : 976 , loss : 0.05378689616918564\n",
      "Epoch : 977 , loss : 0.05373004451394081\n",
      "Epoch : 978 , loss : 0.05367596074938774\n",
      "Epoch : 979 , loss : 0.053616397082805634\n",
      "Epoch : 980 , loss : 0.053564686328172684\n",
      "Epoch : 981 , loss : 0.05350703001022339\n",
      "Epoch : 982 , loss : 0.05345400422811508\n",
      "Epoch : 983 , loss : 0.05339803174138069\n",
      "Epoch : 984 , loss : 0.053342126309871674\n",
      "Epoch : 985 , loss : 0.053289879113435745\n",
      "Epoch : 986 , loss : 0.05323537811636925\n",
      "Epoch : 987 , loss : 0.0531802624464035\n",
      "Epoch : 988 , loss : 0.05312221497297287\n",
      "Epoch : 989 , loss : 0.05307099223136902\n",
      "Epoch : 990 , loss : 0.053018245846033096\n",
      "Epoch : 991 , loss : 0.052960410714149475\n",
      "Epoch : 992 , loss : 0.052911147475242615\n",
      "Epoch : 993 , loss : 0.05285051092505455\n",
      "Epoch : 994 , loss : 0.05280636250972748\n",
      "Epoch : 995 , loss : 0.05274791643023491\n",
      "Epoch : 996 , loss : 0.052692119032144547\n",
      "Epoch : 997 , loss : 0.05264344811439514\n",
      "Epoch : 998 , loss : 0.052585069090127945\n",
      "Epoch : 999 , loss : 0.05253817141056061\n",
      "Epoch : 1000 , loss : 0.05248013883829117\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(x_train_tensor.shape[1])\n",
    " ## defining optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x_train_tensor) ## forward pass \n",
    "    loss = loss_function(y_pred,y_train_tensor.view(-1,1)) ## compute loss\n",
    "    optimizer.zero_grad() ## zero gradients\n",
    "    loss.backward()\n",
    "    optimizer.step() ## update parameters\n",
    "    \n",
    "    print(f'Epoch : {epoch+1} , loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d360d",
   "metadata": {},
   "source": [
    "##### here model.parameters() is the iterator that will iterate over all the trainable parameters of the neural networks.\n",
    "\n",
    "##### torch.optim is the optimizer class that contains various types of optimizers like SGD,Adam,RMSProp etc. , so instead of applying gradient decent and updation of parameters manually torch.optim is used to automate the backprop weight updation process\n",
    "\n",
    "##### also we have the inbuilt loss functions in torch.nn module , which will reduce the effort of creating the loss function manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34eb6808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 51.246535778045654 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f'Test Accuracy : {accuracy.item()*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3895ec",
   "metadata": {},
   "source": [
    "## Using Torch.nn for creating a neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b9bb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a31a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,input_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_features,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,features):\n",
    "        output = self.linear(features)\n",
    "        output = self.sigmoid(output)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f46f33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4279],\n",
       "        [0.5057],\n",
       "        [0.4232],\n",
       "        [0.5201],\n",
       "        [0.4693],\n",
       "        [0.5374],\n",
       "        [0.4479],\n",
       "        [0.4861],\n",
       "        [0.4513],\n",
       "        [0.3894]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.rand(10,5)\n",
    "model = Model(features.shape[1])\n",
    "model(features) ## forward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee61ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4140, -0.3101,  0.3940,  0.3329, -0.2601]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight ## weights of linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "043c995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0598], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias ## bias of linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1598e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31c004c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 1]                   6\n",
       "├─Sigmoid: 1-2                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model,input_size=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40debb80",
   "metadata": {},
   "source": [
    "### Creating NN with 2 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae5b688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,input_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_features,3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3,3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output = nn.Linear(3,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,features):\n",
    "        x = self.linear1(features)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52ec848b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5359],\n",
       "        [0.5359],\n",
       "        [0.5359],\n",
       "        [0.5368],\n",
       "        [0.5359],\n",
       "        [0.5372],\n",
       "        [0.5390],\n",
       "        [0.5359],\n",
       "        [0.5434],\n",
       "        [0.5370]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.rand(10,5)\n",
    "model = Model(input_features = features.shape[1])\n",
    "model(features) ## forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4f26572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3660, -0.1713,  0.1848, -0.3772,  0.3870],\n",
       "        [ 0.1989,  0.1735,  0.3409, -0.2454,  0.1299],\n",
       "        [ 0.3156,  0.1360, -0.2222, -0.2141, -0.3089]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6022f8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5714,  0.3959,  0.1753],\n",
       "        [ 0.4405, -0.2500, -0.0285],\n",
       "        [ 0.1696,  0.5660, -0.0090]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a69b10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 3]                   18\n",
       "├─ReLU: 1-2                              [10, 3]                   --\n",
       "├─Linear: 1-3                            [10, 3]                   12\n",
       "├─ReLU: 1-4                              [10, 3]                   --\n",
       "├─Linear: 1-5                            [10, 1]                   4\n",
       "├─Sigmoid: 1-6                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 34\n",
       "Trainable params: 34\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791a9f6",
   "metadata": {},
   "source": [
    "## Using Sequential Container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079c55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self,input_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_features,3), ## first hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,3), ## second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3,4), ## third hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,1), ## output layer\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,features):\n",
    "        x = self.network(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "680443f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4863],\n",
       "        [0.4749],\n",
       "        [0.4648],\n",
       "        [0.4676],\n",
       "        [0.4685],\n",
       "        [0.4667],\n",
       "        [0.4757],\n",
       "        [0.4769],\n",
       "        [0.4664],\n",
       "        [0.4600]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.rand(10,5)\n",
    "model1 = Model1(input_features=5)\n",
    "model1(features) ## forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ee14989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model1                                   [10, 1]                   --\n",
       "├─Sequential: 1-1                        [10, 1]                   --\n",
       "│    └─Linear: 2-1                       [10, 3]                   18\n",
       "│    └─ReLU: 2-2                         [10, 3]                   --\n",
       "│    └─Linear: 2-3                       [10, 3]                   12\n",
       "│    └─ReLU: 2-4                         [10, 3]                   --\n",
       "│    └─Linear: 2-5                       [10, 4]                   16\n",
       "│    └─ReLU: 2-6                         [10, 4]                   --\n",
       "│    └─Linear: 2-7                       [10, 1]                   5\n",
       "│    └─Sigmoid: 2-8                      [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 51\n",
       "Trainable params: 51\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model1,input_size=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eea454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c7946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f9971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d226c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
